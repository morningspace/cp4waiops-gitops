---
###################################
# CP4WAIOps - Common Configs
###################################
spec:

  ## cp4waiops_namespace is the project (namespace) that you want to create the AI Manager instance in.
  ## You must create a custom project (namespace) and not use the default, kube-system,
  ## kube-public, openshift-node, openshift-infra, or openshift projects (namespaces). 
  ## This is because AI Manager uses Security Context Constraints (SCC), 
  ## and SCCs cannot be assigned to pods created in one of the default OpenShift projects (namespaces).
  ##
  cp4waiops_namespace: cp4waiops

  ## AI Manager catalog source image
  ##
  imageCatalog: icr.io/cpopen/ibm-operator-catalog:latest

  ## A channel defines a stream of updates for an Operator and is used to roll out updates for subscribers. 
  ## For example, if you want to install CP4WAIOps 3.2, the channel should be v3.2
  ##
  channel: v3.2

  ## size is the size that you require for your AI Manager installation. It can be small or large.
  ## More information: https://www.ibm.com/docs/en/cloud-paks/cloud-pak-watson-aiops/3.2.0?topic=requirements-ai-manager
  size: small

  ## dockerUsername is the usrname of IBM® Entitled Registry.
  ## It is used to create a docker-registry secret to enable your deployment to pull the AI Manager images 
  ## from the IBM® Entitled Registry.
  ## Default is cp
  dockerUsername: cp

  ## Obtain the entitlement key that is assigned to your IBMid. 
  ## Log in to MyIBM Container Software Library: https://myibm.ibm.com/products-services/containerlibrary
  ## Opens in a new tab with the IBMid and password details 
  ## that are associated with the entitled software.
  ## 
  dockerPassword: REPLACE_BY_YOUR_OWN_PASSWORD

  ## storageClass is the storage class that you want to use. 
  ## If the storage provider for your deployment is Red Hat OpenShift Data Foundation, 
  ## previously called Red Hat OpenShift Container Storage, then set this to ocs-storagecluster-cephfs
  ##
  storageClass: rook-cephfs

  ## If the storage provider for your deployment is Red Hat OpenShift Data Foundation, 
  ## previously called Red Hat OpenShift Container Storage, then set this to ocs-storagecluster-ceph-rbd
  storageClassLargeBlock: rook-cephfs

res:
  cassandra:
    replicas: 1
    limits:
      cpu: 600m
      memory: 1500Mi
    requests:
      cpu: 200m
      memory: 1500Mi

  couchdb:
    db:
      limits:
        memory: "768Mi"
        cpu: "700m"
      requests:
        memory: "768Mi"
        cpu: "250m"
    search:
      limits:
        memory: "250Mi"
        cpu: "500m"
      requests:
        memory: "250Mi"
        cpu: "250m"
    mgmt:
      limits:
        memory: "128Mi"
        cpu: "500m"
      requests:
        memory: "64Mi"
        cpu: "250m"

  redis:
    member:
      db:
        limits:
          cpu: "500m"
          memory: 512Mi
        requests:
          cpu: "60m"
          memory: 256Mi
      mgmt:
        limits:
          cpu: "500m"
          memory: 150Mi
        requests:
          cpu: "60m"
          memory: 100Mi
      proxy:
        limits:
          cpu: "500m"
          memory: 150Mi
        requests:
          cpu: "60m"
          memory: 100Mi
      proxylog:
        limits:
          cpu: "500m"
          memory: 150Mi
        requests:
          cpu: "60m"
          memory: 100Mi
    sentinel:
      db:
        limits:
          cpu: "500m"
          memory: 512Mi
        requests:
          cpu: "60m"
          memory: 256Mi
      mgmt:
        limits:
          cpu: "500m"
          memory: 150Mi
        requests:
          cpu: "60m"
          memory: 100Mi
      proxy:
        limits:
          cpu: "500m"
          memory: 150Mi
        requests:
          cpu: "60m"
          memory: 100Mi
      proxylog:
        limits:
          cpu: "500m"
          memory: 150Mi
        requests:
          cpu: "60m"
          memory: 100Mi

  lifecycleservice:
    logstash:
      limits:
        cpu: 625m
        memory: 1Gi
      requests:
        cpu: 250m
        memory: 600Mi
    taskManager:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 250m
        memory: 1Gi
    jobManager:
      limits:
        cpu: 600m
        memory: 768Mi
      requests:
        cpu: 100m
        memory: 512Mi

  probablecause:
    replicas: 1
    requests:
      cpu: 800m
      memory: 750Mi
    limits:
      cpu: 1000m
      memory: 1Gi
  classifier:
    replicas: 1
    requests:
      cpu: 800m
      memory: 750Mi
    limits:
      cpu: "1"
      memory: 1000Mi
  spark-worker:
    replicas: 1
    requests:
      cpu: 350m
      memory: 1000Mi
    limits:
      cpu: "1"
      memory: 1000Mi
  spark-pipeline-composer:
    replicas: 1
    requests:
      cpu: 250m
      memory: 500Mi
    limits:
      cpu: 250m
      memory: 500Mi

  ncodl-api:
    replicas: 1
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1024Mi
  ncodl-if:
    replicas: 1
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1024Mi
  ncodl-ir:
    replicas: 1
    requests:
      cpu: 50m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1024Mi
  ncodl-jobmgr:
    replicas: 1
    requests:
      cpu: 50m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1024Mi
  ncodl-std:
    replicas: 1
    requests:
      cpu: 50m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1024Mi
  logstash:
    replicas: 1
    requests:
      cpu: 100m
      memory: 700Mi
    limits:
      cpu: 500m
      memory: 1400Mi
  ncobackup:
    replicas: 1
    containers:
    - name: agg-gate
      requests:
        cpu: 50m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi
    - name: objserv
      requests:
        cpu: 100m
        memory: 512Mi
      limits:
        cpu: 500m
        memory: 1024Mi
  ncoprimary:
    replicas: 1
    requests:
      cpu: 125m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1024Mi  
  